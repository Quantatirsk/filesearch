# LLM Configuration for Smart File Assistant
# Copy this file to .env and configure your LLM API settings

# OpenAI API Configuration
# Required: Your OpenAI API key for LLM functionality
OPENAI_API_KEY=

# Optional: Custom OpenAI API base URL (defaults to https://api.openai.com/v1)
# Use this for OpenAI-compatible services like Azure OpenAI, local LLM servers, etc.
OPENAI_BASE_URL=

# Examples for different LLM providers:
# 
# Azure OpenAI:
# OPENAI_API_KEY=your_azure_openai_key
# OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment/
#
# Local LLM (e.g., Ollama, LM Studio):
# OPENAI_API_KEY=not-required-for-local
# OPENAI_BASE_URL=http://localhost:11434/v1
#
# Other OpenAI-compatible services:
# OPENAI_API_KEY=your_service_api_key
# OPENAI_BASE_URL=https://your-service.com/v1

# Notes:
# 1. The backend must be restarted after changing these variables
# 2. Without a valid API key, LLM features will be disabled
# 3. File search functionality works independently of LLM configuration